{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data\\\\CleanTwitter.csv', 'r', newline='') as clean:\n",
    "    read = csv.reader(clean, delimiter=',')\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    extended_stops = {\"ya\", \"yo\", \"yu\", \"da\", \"em\", \"im\", \"theres\", \"dat\", \"dats\", \"aint\", \"thats\", \"doe\", \"ur\"}\n",
    "    negatives = [\"no\", \"none\", \"not\"]\n",
    "    \n",
    "    with open('data\\\\TwitterPreprocessed.csv', 'w', newline='') as prep:\n",
    "        write = csv.writer(prep)\n",
    "        \n",
    "        prev = -1\n",
    "        progress = 0\n",
    "        \n",
    "        for idx,i in enumerate(read):\n",
    "\n",
    "            word_tokens = nltk.word_tokenize(i[1])\n",
    "            filtered_sentence = []\n",
    "            \n",
    "            for w in word_tokens:\n",
    "                if w not in stop_words and w not in extended_stops and len(w) > 1 or w in negatives:\n",
    "                    filtered_sentence.append(w)\n",
    "            \n",
    "            filtered_sentence = [j[0] for j in groupby(filtered_sentence)]\n",
    "            filtered_sentence = TreebankWordDetokenizer().detokenize(filtered_sentence)\n",
    "            \n",
    "            if filtered_sentence:\n",
    "                write.writerow((i[0], filtered_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data\\\\corpus.csv', 'r') as corp:\n",
    "    read = csv.reader(corp)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    extended_stops = {\"ya\", \"yo\", \"yu\", \"da\", \"em\", \"im\", \"theres\", \"dat\", \"dats\", \"aint\", \"thats\", \"doe\", \"ur\"}\n",
    "    negatives = [\"no\", \"none\", \"not\"]\n",
    "    \n",
    "    df = []\n",
    "    \n",
    "    for idx,i in enumerate(read):\n",
    "        \n",
    "        tokens = nltk.word_tokenize(i[1])\n",
    "        filtered = []\n",
    "        \n",
    "        for w in tokens:\n",
    "            if w not in stop_words and w not in extended_stops and len(w) > 1 or w in negatives:\n",
    "                filtered.append(w)\n",
    "        df.append(filtered)\n",
    "        \n",
    "    \n",
    "    with open('data\\\\allTokens.csv', 'w', newline='') as tok:\n",
    "        write = csv.writer(tok)\n",
    "        \n",
    "        for i in df:\n",
    "            write.writerow((i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data\\\\allTokens.csv', 'r') as wordsList:\n",
    "    read = csv.reader(wordsList)\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    for i in read:\n",
    "        words.append(i)\n",
    "    vector_model = Word2Vec(sentences=words, size=300, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import *\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/TwitterPreprocessed.csv', 'r') as dataList:\n",
    "    read = csv.reader(dataList)\n",
    "    \n",
    "    hate = 0\n",
    "    noHate = 0\n",
    "    \n",
    "    for i in read:\n",
    "        if i[0] == '1':\n",
    "            hate += 1\n",
    "        else:\n",
    "            noHate += 1\n",
    "\n",
    "with open('data/TwitterPreprocessed.csv', 'r') as dataList:\n",
    "    read = csv.reader(dataList)\n",
    "    tokenizer = Tokenizer(num_words=12000, oov_token=\"<UNK>\")\n",
    "    \n",
    "    maxlen = 128\n",
    "    all_data = []\n",
    "    \n",
    "    x_train_text = []\n",
    "    x_train = []\n",
    "    \n",
    "    y_train = np.array([])\n",
    "    y_train = y_train.astype('int64') \n",
    "    \n",
    "    x_test_text = []\n",
    "    x_test = []\n",
    "    \n",
    "    y_test = np.array([])\n",
    "    y_test = y_test.astype('int64') \n",
    "    \n",
    "    iter1 = 0\n",
    "    iter2 = 0\n",
    "    \n",
    "    for i,j in read:\n",
    "        if i == '1':\n",
    "            iter1 += 1\n",
    "            \n",
    "            if iter1 <= round(hate*0.8):\n",
    "                x_train_text.append(j)\n",
    "                y_train = np.append(y_train, int(i))\n",
    "            else:\n",
    "                x_test_text.append(j)\n",
    "                y_test = np.append(y_test, int(i))\n",
    "        \n",
    "        elif i == '0':\n",
    "            iter2 += 1\n",
    "            \n",
    "            if iter2 <= round(noHate*0.8):\n",
    "                x_train_text.append(j)\n",
    "                y_train = np.append(y_train, int(i))\n",
    "            else:\n",
    "                x_test_text.append(j)\n",
    "                y_test = np.append(y_test, int(i))\n",
    "        \n",
    "        all_data.append(j)\n",
    "    \n",
    "    tokenizer.fit_on_texts(all_data)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    train_sequences = tokenizer.texts_to_sequences(x_train_text)\n",
    "    x_train = pad_sequences(train_sequences, padding='post', truncating='post', maxlen=maxlen)\n",
    "    \n",
    "    test_sequences = tokenizer.texts_to_sequences(x_test_text)\n",
    "    x_test = pad_sequences(test_sequences, padding='post', truncating='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_index, open(\"pickles\\\\word_index.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Activation, Dropout, Bidirectional, Input, Concatenate, Reshape\n",
    "from tensorflow.keras.layers import LSTM, Conv1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras import regularizers\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initWeights():\n",
    "    zero_vec = np.array([])\n",
    "    zero_vec.astype('float32')\n",
    "\n",
    "    for i in vector_model[\"random\"]:\n",
    "        zero_vec = np.append(zero_vec, 0)\n",
    "\n",
    "    embedding_weights = np.zeros((max_features, dim_count))\n",
    "    for word,index in word_index.items():\n",
    "        try:\n",
    "            embedding_weights[index, :] = vector_model[word]\n",
    "        except:\n",
    "            embedding_weights[index, :] = zero_vec\n",
    "            \n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n",
      "C:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7141 samples, validate on 1785 samples\n",
      "Epoch 1/20\n",
      "7141/7141 [==============================] - 430s 60ms/sample - loss: 3.4164 - accuracy: 0.7944 - precision_2: 0.6972 - recall_2: 0.8522 - val_loss: 0.9362 - val_accuracy: 0.8868 - val_precision_2: 0.8172 - val_recall_2: 0.8321\n",
      "Epoch 2/20\n",
      "7141/7141 [==============================] - 421s 59ms/sample - loss: 0.9380 - accuracy: 0.8663 - precision_2: 0.8383 - recall_2: 0.8366 - val_loss: 0.5737 - val_accuracy: 0.9154 - val_precision_2: 0.8591 - val_recall_2: 0.8482\n",
      "Epoch 3/20\n",
      "7141/7141 [==============================] - 428s 60ms/sample - loss: 0.7238 - accuracy: 0.8695 - precision_2: 0.8630 - recall_2: 0.8518 - val_loss: 0.5776 - val_accuracy: 0.8852 - val_precision_2: 0.8723 - val_recall_2: 0.8512\n",
      "Epoch 4/20\n",
      "7141/7141 [==============================] - 422s 59ms/sample - loss: 0.6525 - accuracy: 0.8765 - precision_2: 0.8753 - recall_2: 0.8525 - val_loss: 0.4695 - val_accuracy: 0.9076 - val_precision_2: 0.8810 - val_recall_2: 0.8574\n",
      "Epoch 5/20\n",
      "7141/7141 [==============================] - 427s 60ms/sample - loss: 0.5896 - accuracy: 0.8804 - precision_2: 0.8810 - recall_2: 0.8595 - val_loss: 0.4766 - val_accuracy: 0.8924 - val_precision_2: 0.8851 - val_recall_2: 0.8612\n",
      "Epoch 6/20\n",
      "7141/7141 [==============================] - 431s 60ms/sample - loss: 0.5833 - accuracy: 0.8783 - precision_2: 0.8847 - recall_2: 0.8626 - val_loss: 0.4238 - val_accuracy: 0.9098 - val_precision_2: 0.8875 - val_recall_2: 0.8642\n",
      "Epoch 7/20\n",
      "7141/7141 [==============================] - 430s 60ms/sample - loss: 0.5423 - accuracy: 0.8798 - precision_2: 0.8875 - recall_2: 0.8649 - val_loss: 0.4173 - val_accuracy: 0.9076 - val_precision_2: 0.8899 - val_recall_2: 0.8659\n",
      "Epoch 8/20\n",
      "7141/7141 [==============================] - 567s 79ms/sample - loss: 0.5207 - accuracy: 0.8804 - precision_2: 0.8901 - recall_2: 0.8666 - val_loss: 0.3873 - val_accuracy: 0.9199 - val_precision_2: 0.8920 - val_recall_2: 0.8683\n",
      "Epoch 9/20\n",
      "7141/7141 [==============================] - 409s 57ms/sample - loss: 0.5040 - accuracy: 0.8803 - precision_2: 0.8915 - recall_2: 0.8690 - val_loss: 0.3553 - val_accuracy: 0.9160 - val_precision_2: 0.8930 - val_recall_2: 0.8704\n",
      "Epoch 10/20\n",
      "7141/7141 [==============================] - 425s 59ms/sample - loss: 0.4698 - accuracy: 0.8805 - precision_2: 0.8926 - recall_2: 0.8709 - val_loss: 0.3893 - val_accuracy: 0.9115 - val_precision_2: 0.8939 - val_recall_2: 0.8715\n",
      "Epoch 11/20\n",
      "7141/7141 [==============================] - 420s 59ms/sample - loss: 0.4620 - accuracy: 0.8856 - precision_2: 0.8939 - recall_2: 0.8720 - val_loss: 0.3624 - val_accuracy: 0.9148 - val_precision_2: 0.8954 - val_recall_2: 0.8725\n",
      "Epoch 12/20\n",
      "7141/7141 [==============================] - 408s 57ms/sample - loss: 0.4569 - accuracy: 0.8850 - precision_2: 0.8956 - recall_2: 0.8727 - val_loss: 0.3467 - val_accuracy: 0.9126 - val_precision_2: 0.8967 - val_recall_2: 0.8733\n",
      "Epoch 13/20\n",
      "7141/7141 [==============================] - 409s 57ms/sample - loss: 0.4273 - accuracy: 0.8881 - precision_2: 0.8967 - recall_2: 0.8739 - val_loss: 0.3315 - val_accuracy: 0.9188 - val_precision_2: 0.8978 - val_recall_2: 0.8746\n",
      "Epoch 14/20\n",
      "7141/7141 [==============================] - 428s 60ms/sample - loss: 0.4279 - accuracy: 0.8878 - precision_2: 0.8977 - recall_2: 0.8751 - val_loss: 0.3123 - val_accuracy: 0.9227 - val_precision_2: 0.8987 - val_recall_2: 0.8756\n",
      "Epoch 15/20\n",
      "7141/7141 [==============================] - 427s 60ms/sample - loss: 0.4182 - accuracy: 0.8888 - precision_2: 0.8988 - recall_2: 0.8759 - val_loss: 0.3253 - val_accuracy: 0.9132 - val_precision_2: 0.8998 - val_recall_2: 0.8763\n",
      "Epoch 16/20\n",
      "7141/7141 [==============================] - 425s 59ms/sample - loss: 0.4011 - accuracy: 0.8855 - precision_2: 0.8996 - recall_2: 0.8763 - val_loss: 0.3347 - val_accuracy: 0.9053 - val_precision_2: 0.9005 - val_recall_2: 0.8762\n",
      "Epoch 17/20\n",
      "7141/7141 [==============================] - 424s 59ms/sample - loss: 0.3914 - accuracy: 0.8880 - precision_2: 0.9007 - recall_2: 0.8762 - val_loss: 0.2935 - val_accuracy: 0.9210 - val_precision_2: 0.9016 - val_recall_2: 0.8767\n",
      "Epoch 18/20\n",
      "7141/7141 [==============================] - 423s 59ms/sample - loss: 0.3896 - accuracy: 0.8922 - precision_2: 0.9014 - recall_2: 0.8770 - val_loss: 0.3058 - val_accuracy: 0.9193 - val_precision_2: 0.9024 - val_recall_2: 0.8773\n",
      "Epoch 19/20\n",
      "7141/7141 [==============================] - 424s 59ms/sample - loss: 0.3764 - accuracy: 0.8915 - precision_2: 0.9024 - recall_2: 0.8774 - val_loss: 0.2885 - val_accuracy: 0.9221 - val_precision_2: 0.9033 - val_recall_2: 0.8776\n",
      "Epoch 20/20\n",
      "7141/7141 [==============================] - 453s 63ms/sample - loss: 0.3700 - accuracy: 0.8926 - precision_2: 0.9035 - recall_2: 0.8778 - val_loss: 0.3056 - val_accuracy: 0.9053 - val_precision_2: 0.9042 - val_recall_2: 0.8779\n",
      "1785/1785 - 6s - loss: 0.3056 - accuracy: 0.9053 - precision_2: 0.9045 - recall_2: 0.8780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.305625716419447, 0.90532213, 0.90446866, 0.8779811]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_count = 300\n",
    "\n",
    "max_features = len(word_index) + 1\n",
    "embedding_weights = initWeights()\n",
    "vocab_len = len(embedding_weights)\n",
    "\n",
    "embedding_input = Input(shape=(128,), name='embedding_input')\n",
    "\n",
    "embedding = Embedding(max_features, dim_count, trainable = False, weights = [embedding_weights])(embedding_input)\n",
    "bilstm = Bidirectional(LSTM(128, dropout=0.4, recurrent_dropout=0.4, return_sequences=True,\n",
    "                            activity_regularizer=regularizers.l1(0.01), kernel_regularizer=regularizers.l2(0.01)))(embedding)\n",
    "lstm = LSTM(128, dropout=0.4, recurrent_dropout=0.4, return_sequences=True)(bilstm)\n",
    "maxPool = GlobalMaxPooling1D()(lstm)\n",
    "averagePool = GlobalAveragePooling1D()(lstm)\n",
    "concat = Concatenate()([maxPool, averagePool])\n",
    "dropout = Dropout(0.2)(concat)\n",
    "dense = Dense(128, activation='relu')(dropout)\n",
    "dense = Dense(64, activation='relu')(dense)\n",
    "out = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "model = Model(inputs=embedding_input, outputs=out)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=20, validation_data=(x_test, y_test), shuffle=True)\n",
    "model.evaluate(x_test, y_test, batch_size=32, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"NN_models\\\\BiLSTM-LSTM_Vinf.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n",
      "C:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7141 samples, validate on 1785 samples\n",
      "Epoch 1/10\n",
      "7141/7141 [==============================] - 94s 13ms/sample - loss: 0.9108 - accuracy: 0.8566 - precision_2: 0.8123 - recall_2: 0.8478 - val_loss: 0.6484 - val_accuracy: 0.8975 - val_precision_2: 0.8827 - val_recall_2: 0.8718\n",
      "Epoch 2/10\n",
      "7141/7141 [==============================] - 87s 12ms/sample - loss: 0.5660 - accuracy: 0.8954 - precision_2: 0.8833 - recall_2: 0.8771 - val_loss: 0.5386 - val_accuracy: 0.8790 - val_precision_2: 0.8963 - val_recall_2: 0.8822\n",
      "Epoch 3/10\n",
      "7141/7141 [==============================] - 92s 13ms/sample - loss: 0.4556 - accuracy: 0.9037 - precision_2: 0.8948 - recall_2: 0.8837 - val_loss: 0.3861 - val_accuracy: 0.9244 - val_precision_2: 0.9040 - val_recall_2: 0.8852\n",
      "Epoch 4/10\n",
      "7141/7141 [==============================] - 84s 12ms/sample - loss: 0.3957 - accuracy: 0.9109 - precision_2: 0.9064 - recall_2: 0.8863 - val_loss: 0.3687 - val_accuracy: 0.9171 - val_precision_2: 0.9122 - val_recall_2: 0.8886\n",
      "Epoch 5/10\n",
      "7141/7141 [==============================] - 85s 12ms/sample - loss: 0.3594 - accuracy: 0.9164 - precision_2: 0.9137 - recall_2: 0.8897 - val_loss: 0.3465 - val_accuracy: 0.9165 - val_precision_2: 0.9181 - val_recall_2: 0.8901\n",
      "Epoch 6/10\n",
      "7141/7141 [==============================] - 85s 12ms/sample - loss: 0.3342 - accuracy: 0.9181 - precision_2: 0.9196 - recall_2: 0.8909 - val_loss: 0.3168 - val_accuracy: 0.9266 - val_precision_2: 0.9228 - val_recall_2: 0.8919\n",
      "Epoch 7/10\n",
      "7141/7141 [==============================] - 89s 13ms/sample - loss: 0.3245 - accuracy: 0.9199 - precision_2: 0.9240 - recall_2: 0.8926 - val_loss: 0.3336 - val_accuracy: 0.9165 - val_precision_2: 0.9263 - val_recall_2: 0.8929\n",
      "Epoch 8/10\n",
      "7141/7141 [==============================] - 87s 12ms/sample - loss: 0.3078 - accuracy: 0.9233 - precision_2: 0.9272 - recall_2: 0.8935 - val_loss: 0.3121 - val_accuracy: 0.9160 - val_precision_2: 0.9293 - val_recall_2: 0.8939\n",
      "Epoch 9/10\n",
      "7141/7141 [==============================] - 88s 12ms/sample - loss: 0.2980 - accuracy: 0.9276 - precision_2: 0.9298 - recall_2: 0.8949 - val_loss: 0.3195 - val_accuracy: 0.9109 - val_precision_2: 0.9316 - val_recall_2: 0.8954\n",
      "Epoch 10/10\n",
      "7141/7141 [==============================] - 87s 12ms/sample - loss: 0.2903 - accuracy: 0.9277 - precision_2: 0.9321 - recall_2: 0.8961 - val_loss: 0.3061 - val_accuracy: 0.9188 - val_precision_2: 0.9335 - val_recall_2: 0.8967\n",
      "1785/1785 - 3s - loss: 0.3061 - accuracy: 0.9188 - precision_2: 0.9337 - recall_2: 0.8968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30614780904699107, 0.9187675, 0.93371445, 0.89681536]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_count = 300\n",
    "\n",
    "max_features = len(word_index) + 1\n",
    "embedding_weights = initWeights()\n",
    "vocab_len = len(embedding_weights)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, dim_count, trainable = False, weights = [embedding_weights]))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True,\n",
    "               activity_regularizer=regularizers.l1(0.001), kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Conv1D(int(128 / 2), 3, kernel_initializer='he_normal', activation='relu', padding='valid',\n",
    "                activity_regularizer=regularizers.l1(0.001), kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Conv1D(int(128 / 4), 3, kernel_initializer='he_normal', activation='relu', padding='valid',\n",
    "                activity_regularizer=regularizers.l1(0.001), kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test), shuffle=True)\n",
    "model.evaluate(x_test, y_test, batch_size=32, verbose = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
